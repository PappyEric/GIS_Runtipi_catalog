# Geospatial AI Suite

Run your own private AI/LLM stack locally.

## Components
- **Ollama**: The backend engine to run models like Llama 3, Mistral, etc.
- **AnythingLLM**: The frontend chat interface with RAG (Retrieval Augmented Generation) capabilities.

## Usage
1. Open AnythingLLM.
2. In settings, configure the "Local AI" provider to point to `http://ollama:11434`.
3. Download models via the UI or CLI.
